{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Deep Learning class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EHRDataloader import EHRdataFromPickles, EHRdataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "This part of the code loads the dataset, we use the EHRDataLoader.py The initial code could be found: https://github.com/ZhiGroup/pytorch_ehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file found. Data will be split into train, validation and test.\n"
     ]
    }
   ],
   "source": [
    "print('1 file found. Data will be split into train, validation and test.')\n",
    "data = EHRdataFromPickles(root_dir = '../data/', \n",
    "                      file = 'toy.train', \n",
    "                      sort= False,\n",
    "                      test_ratio = 0.2, \n",
    "                      valid_ratio = 0.1,\n",
    "                      model='RNN') #No sort before splitting\n",
    "\n",
    "# Dataloader splits\n",
    "train, test, valid = data.__splitdata__() #this time, sort is true\n",
    "# can comment out this part if you dont want to know what's going on here\n",
    "# print(colored(\"\\nSee an example data structure from training data:\", 'green'))\n",
    "# print(data.__getitem__(35, seeDescription = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the patients labels, where 1: heart failure and 0: no heart failure\n",
    "labels=[]\n",
    "for ii in range(len(train)):\n",
    "    label=train[ii][1]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of the labels\n",
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print sizes of train test and valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 2000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test),len(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 1873\n",
      "Heart Failure: 1\n",
      "# of visits: 141\n",
      " list of visit_time (since last time): [17739]\n",
      " list of codes corresponding to visit: [3886, 6080, 6595, 4148, 373, 7530, 18327, 13971, 16273, 8777, 5682, 342, 4479, 8031, 11130, 19166, 7315, 12275, 12988, 8735, 647, 19515, 11728, 11449, 17860, 15215, 17746, 14643, 5422, 17009, 16438, 16686]\n"
     ]
    }
   ],
   "source": [
    "## example dataset for patient 0 and visit 0\n",
    "patient=2\n",
    "visit=0\n",
    "print(\"Patient ID:\", train[patient][0])\n",
    "print(\"Heart Failure:\", train[patient][1])\n",
    "print(\"# of visits:\", len(train[patient][2]))\n",
    "\n",
    "print(f' list of visit_time (since last time): {train[patient][2][visit][0]}')\n",
    "print(f' list of codes corresponding to visit: {train[patient][2][visit][1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data for Training\n",
    "\n",
    "This part of the code transforms the data which has the format described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pids: contains the patient ids\n",
    "- vids: contains a list of visit ids for each patient\n",
    "- hfs: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- seqs: contains a list of visit (in ICD9 codes) for each patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data for train set\n",
    "aux=[len(train[ii][2]) for ii in range(len(train))]\n",
    "vids=[list(np.arange(ii)) for ii in aux]\n",
    "hfs=[train[ii][1] for ii in range(len(train))]\n",
    "\n",
    "seqs=[]\n",
    "for ii in range(len(train)):\n",
    "    patient=[]\n",
    "    for visit in range(len(train[ii][2])):\n",
    "        patient.append(train[ii][2][visit][1])\n",
    "    seqs.append(patient)\n",
    "    \n",
    "pids=[train[ii][0] for ii in range(len(train))]\n",
    "assert len(pids) == len(vids) == len(hfs) == len(seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data for val set\n",
    "aux_val=[len(valid[ii][2]) for ii in range(len(valid))]\n",
    "vids_val=[list(np.arange(ii)) for ii in aux_val]\n",
    "hfs_val=[valid[ii][1] for ii in range(len(valid))]\n",
    "\n",
    "seqs_val=[]\n",
    "for ii in range(len(valid)):\n",
    "    patient=[]\n",
    "    for visit in range(len(valid[ii][2])):\n",
    "        patient.append(valid[ii][2][visit][1])\n",
    "    seqs_val.append(patient)\n",
    "    \n",
    "pids_val=[valid[ii][0] for ii in range(len(valid))]\n",
    "assert len(pids_val) == len(vids_val) == len(hfs_val) == len(seqs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data for test set\n",
    "\n",
    "aux_test=[len(test[ii][2]) for ii in range(len(test))]\n",
    "vids_test=[list(np.arange(ii)) for ii in aux_test]\n",
    "hfs_test=[test[ii][1] for ii in range(len(test))]\n",
    "\n",
    "seqs_test=[]\n",
    "for ii in range(len(test)):\n",
    "    patient=[]\n",
    "    for visit in range(len(test[ii][2])):\n",
    "        patient.append(test[ii][2][visit][1])\n",
    "    seqs_test.append(patient)\n",
    "    \n",
    "pids_test=[test[ii][0] for ii in range(len(test))]\n",
    "\n",
    "assert len(pids_test) == len(vids_test) == len(hfs_test) == len(seqs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, hfs):\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "    \n",
    "    def __len__(self):\n",
    "    \n",
    "        # your code here\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # your code here\n",
    "        return self.x[index],self.y[index]\n",
    "        \n",
    "train_dataset = CustomDataset(seqs, hfs)\n",
    "\n",
    "test_dataset = CustomDataset(seqs_test, hfs_test)\n",
    "\n",
    "val_dataset= CustomDataset(seqs_val, hfs_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            for ii,code in enumerate(visit):\n",
    "                \n",
    "                x[i_patient,j_visit,ii]=code\n",
    "                masks[i_patient,j_visit,ii]=1\n",
    "             \n",
    "\n",
    "        for j_visit, visit in enumerate(patient[::-1]):\n",
    "            for ii,code in enumerate(visit):\n",
    "                rev_x[i_patient,j_visit,ii]=code\n",
    "                rev_masks[i_patient,j_visit,ii]=1\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a batch sample from the train set\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 154 max number of visits\n",
    "## 49 max number of diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 154, 49]),\n",
       " torch.Size([32, 154, 49]),\n",
       " torch.Size([32, 154, 49]),\n",
       " torch.Size([32, 154, 49]),\n",
       " torch.Size([32]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, masks.shape, rev_x.shape, rev_masks.shape, y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 7000\n",
      "Length of val dataset: 1000\n",
      "Length of test dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset,test_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 64\n",
    "    # your code here\n",
    "    train_loader=DataLoader(train_dataset,collate_fn=collate_fn,batch_size=batch_size,shuffle=True)\n",
    "    val_loader=DataLoader(val_dataset,collate_fn=collate_fn,batch_size=batch_size,shuffle=False)\n",
    "    test_loader=DataLoader(test_dataset,collate_fn=collate_fn,batch_size=batch_size,shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_loader, val_loader,test_loader\n",
    "\n",
    "## We get the train, val and test Data Loaders\n",
    "train_loader, val_loader,test_loader = load_data(train_dataset, val_dataset,test_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train RETAIN Model\n",
    "\n",
    "Training the model, note that we are not using any pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        TODO: Implement the alpha attention.\n",
    "        \n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "            \n",
    "        HINT: consider `torch.softmax`\n",
    "        \"\"\"\n",
    "        \n",
    "        alpha=torch.softmax(self.a_att(g),dim=1)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.b_att` for beta-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        TODO: Implement the beta attention.\n",
    "        \n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            \n",
    "        HINT: consider `torch.tanh`\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        beta=torch.tanh(self.b_att(h))\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the hidden states for true visits (not padding visits) and then\n",
    "        sum the them up.\n",
    "\n",
    "    Arguments:\n",
    "        alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "        beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "        rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "        rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes) (bath_size,# visits, #diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    out=torch.unsqueeze(torch.max(rev_masks,dim=2).values,-1) * rev_v\n",
    "    \n",
    "    out=out*alpha*beta\n",
    "    \n",
    "    out=torch.sum(out,dim=1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We get the total number of unique codes\n",
    "all_codes=[]\n",
    "for patient in seqs:\n",
    "    for visit in patient:\n",
    "        all_codes.extend(visit)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        # Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
    "        # Define the RNN-alpha using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the RNN-beta using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the alpha-attention using `AlphaAttention()`;\n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        # Define the beta-attention using `BetaAttention()`;\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        # Define the linear layers using `nn.Linear()`;\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        # Define the final activation layer using `nn.Sigmoid().\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            rev_x: the diagnosis sequence in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        # 1. Pass the reversed sequence through the embedding layer;\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        # 2. Sum the reversed embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        # 3. Pass the reversed embegginds through the RNN-alpha and RNN-beta layer separately;\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        # 4. Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        # 5. Sum the attention up using `attention_sum()`;\n",
    "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
    "        # 6. Pass the context vector through the linear and activation layers.\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_logit = model(x, masks, rev_x, rev_masks)\n",
    "        \"\"\"\n",
    "        TODO: obtain the predicted class (0, 1) by comparing y_logit against 0.5, \n",
    "              assign the predicted class to y_hat.\n",
    "        \"\"\"\n",
    "        y_hat = (y_logit>0.5).int()\n",
    "        # your code here\n",
    "#         raise NotImplementedError\n",
    "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader,test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            \"\"\" \n",
    "            TODO: calculate the loss using `criterion`, save the output to loss.\n",
    "            \"\"\"\n",
    "            loss = criterion(y_hat,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "        p, r, f, roc_auc = eval(model, test_loader)\n",
    "        print('Epoch: {} \\t Test p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "        print('\\n')\n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results RETAIN MODEL\n",
    "\n",
    "Here you will find the table of the results for the RNN GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.693934\n",
      "Epoch: 1 \t Validation p: 0.53, r:0.41, f: 0.46, roc_auc: 0.52\n",
      "Epoch: 1 \t Test p: 0.52, r:0.38, f: 0.44, roc_auc: 0.51\n",
      "\n",
      "\n",
      "Epoch: 2 \t Training Loss: 0.546473\n",
      "Epoch: 2 \t Validation p: 0.54, r:0.53, f: 0.54, roc_auc: 0.53\n",
      "Epoch: 2 \t Test p: 0.51, r:0.52, f: 0.52, roc_auc: 0.50\n",
      "\n",
      "\n",
      "Epoch: 3 \t Training Loss: 0.155120\n",
      "Epoch: 3 \t Validation p: 0.53, r:0.53, f: 0.53, roc_auc: 0.53\n",
      "Epoch: 3 \t Test p: 0.52, r:0.52, f: 0.52, roc_auc: 0.51\n",
      "\n",
      "\n",
      "Epoch: 4 \t Training Loss: 0.034107\n",
      "Epoch: 4 \t Validation p: 0.54, r:0.53, f: 0.54, roc_auc: 0.53\n",
      "Epoch: 4 \t Test p: 0.52, r:0.51, f: 0.51, roc_auc: 0.50\n",
      "\n",
      "\n",
      "Epoch: 5 \t Training Loss: 0.014557\n",
      "Epoch: 5 \t Validation p: 0.54, r:0.54, f: 0.54, roc_auc: 0.53\n",
      "Epoch: 5 \t Test p: 0.51, r:0.51, f: 0.51, roc_auc: 0.50\n",
      "\n",
      "\n",
      "Epoch: 6 \t Training Loss: 0.009038\n",
      "Epoch: 6 \t Validation p: 0.54, r:0.54, f: 0.54, roc_auc: 0.53\n",
      "Epoch: 6 \t Test p: 0.51, r:0.51, f: 0.51, roc_auc: 0.50\n",
      "\n",
      "\n",
      "Epoch: 7 \t Training Loss: 0.006316\n",
      "Epoch: 7 \t Validation p: 0.54, r:0.53, f: 0.53, roc_auc: 0.53\n",
      "Epoch: 7 \t Test p: 0.52, r:0.51, f: 0.52, roc_auc: 0.50\n",
      "\n",
      "\n",
      "Epoch: 8 \t Training Loss: 0.004663\n",
      "Epoch: 8 \t Validation p: 0.54, r:0.53, f: 0.54, roc_auc: 0.53\n",
      "Epoch: 8 \t Test p: 0.52, r:0.52, f: 0.52, roc_auc: 0.50\n",
      "\n",
      "\n",
      "Epoch: 9 \t Training Loss: 0.003522\n",
      "Epoch: 9 \t Validation p: 0.53, r:0.53, f: 0.53, roc_auc: 0.53\n",
      "Epoch: 9 \t Test p: 0.52, r:0.52, f: 0.52, roc_auc: 0.50\n",
      "\n",
      "\n",
      "Epoch: 10 \t Training Loss: 0.002827\n",
      "Epoch: 10 \t Validation p: 0.53, r:0.53, f: 0.53, roc_auc: 0.53\n",
      "Epoch: 10 \t Test p: 0.51, r:0.51, f: 0.51, roc_auc: 0.50\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "retain = RETAIN(num_codes = len(set(all_codes))+1,embedding_dim=128)\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(retain.parameters(), lr=0.001)\n",
    "\n",
    "# optimizer = torch.optim.RMSprop(retain.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.SGD(retain.parameters(), \n",
    "#                       lr=0.001,\n",
    "#                       weight_decay=0.9)\n",
    "\n",
    "n_epochs = 10\n",
    "train(retain, train_loader, val_loader,test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0 1.0\n",
      "0.5126953125 0.5097087378640777 0.5111976630963972 0.5048233410069062\n"
     ]
    }
   ],
   "source": [
    "# after model training\n",
    "p, r, f, roc_auc = eval(retain, train_loader)\n",
    "print(p, r, f, roc_auc)\n",
    "\n",
    "\n",
    "p, r, f, roc_auc = eval(retain, test_loader)\n",
    "print(p, r, f, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4987075928917609 0.8792366847052122 0.6364292340995773 0.48944193652851586\n",
      "0.5 0.525242718446602 0.5123106060606061 0.4896987288559704\n"
     ]
    }
   ],
   "source": [
    "# without model training\n",
    "retain = RETAIN(num_codes = len(set(all_codes))+1)\n",
    "\n",
    "p, r, f, roc_auc = eval(retain, train_loader)\n",
    "print(p, r, f, roc_auc)\n",
    "\n",
    "\n",
    "p, r, f, roc_auc = eval(retain, test_loader)\n",
    "print(p, r, f, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
