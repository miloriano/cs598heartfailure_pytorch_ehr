{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EHRDataloader import EHRdataFromPickles, EHRdataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "This part of the code loads the dataset, we use the EHRDataLoader.py\n",
    "The initial code could be found: https://github.com/ZhiGroup/pytorch_ehr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file found. Data will be split into train, validation and test.\n"
     ]
    }
   ],
   "source": [
    "print('1 file found. Data will be split into train, validation and test.')\n",
    "data = EHRdataFromPickles(root_dir = '../data/', \n",
    "                      file = 'toy.train', \n",
    "                      sort= False,\n",
    "                      test_ratio = 0.2, \n",
    "                      valid_ratio = 0.1,\n",
    "                      model='RNN') #No sort before splitting\n",
    "\n",
    "# Dataloader splits\n",
    "train, test, valid = data.__splitdata__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the patients labels, where 1: heart failure and 0: no heart failure\n",
    "labels=[]\n",
    "for ii in range(len(train)):\n",
    "    label=train[ii][1]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3511, 0: 3489})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Distribution of the labels\n",
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print sizes of train test and valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 2000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test),len(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 2569\n",
      "Heart Failure: 1\n",
      "# of visits: 154\n",
      " list of visit_time (since last time): [12753]\n",
      " list of codes corresponding to visit: [2836]\n"
     ]
    }
   ],
   "source": [
    "## example dataset for patient 0 and visit 0\n",
    "patient=0\n",
    "visit=0\n",
    "print(\"Patient ID:\", train[patient][0])\n",
    "print(\"Heart Failure:\", train[patient][1])\n",
    "print(\"# of visits:\", len(train[patient][2]))\n",
    "\n",
    "print(f' list of visit_time (since last time): {train[patient][2][visit][0]}')\n",
    "print(f' list of codes corresponding to visit: {train[patient][2][visit][1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data for Training\n",
    "\n",
    "This part of the code transforms the data which has the format described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "pack_pad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:03<00:00, 66.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " creating the list of valid minibatches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 66.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " creating the list of test minibatches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:00<00:00, 66.13it/s]\n"
     ]
    }
   ],
   "source": [
    "## Understand EHRdataloader\n",
    "train_mbs = list(tqdm(EHRdataloader(train, batch_size = batch_size, packPadMode = pack_pad)))\n",
    "print (' creating the list of valid minibatches')\n",
    "valid_mbs = list(tqdm(EHRdataloader(valid, batch_size = batch_size, packPadMode = pack_pad)))\n",
    "print (' creating the list of test minibatches')\n",
    "test_mbs = list(tqdm(EHRdataloader(test, batch_size = batch_size, packPadMode = pack_pad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Recurrent Neural Network\n",
    "\n",
    "Training the model, note that we are not using any pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F \n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "#construct a whole embedding class from pytorch nn.module\n",
    "#then we call this class in models after we define it \n",
    "class EHREmbeddings(nn.Module):\n",
    "    #initialization and then the forward and things\n",
    "    #DRNN has no bi, QRNN no bi, TLSTM has no bi, but DRNN has other cell-types \n",
    "    #cel_type are different for each model variation \n",
    "    def __init__(self, input_size, embed_dim ,hidden_size, n_layers=1,dropout_r=0.1,cell_type='LSTM', bii=False, time=False , preTrainEmb='', packPadMode = True):\n",
    "        super(EHREmbeddings, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cell_type = cell_type\n",
    "        self.time=time\n",
    "        self.preTrainEmb=preTrainEmb\n",
    "        self.packPadMode = packPadMode\n",
    "        if bii: \n",
    "            self.bi=2 \n",
    "        else: \n",
    "            self.bi=1\n",
    "            \n",
    "        if len(input_size)==1:\n",
    "            self.multi_emb=False\n",
    "            if len(self.preTrainEmb)>0:\n",
    "                emb_t= torch.FloatTensor(np.asmatrix(self.preTrainEmb))\n",
    "                self.embed= nn.Embedding.from_pretrained(emb_t)#,freeze=False) \n",
    "                self.in_size= embed_dim ### need to be updated to be automatically capyured from the input\n",
    "            else:\n",
    "                input_size=input_size[0]\n",
    "                self.embed= nn.Embedding(input_size, self.embed_dim,padding_idx=0)#,scale_grad_by_freq=True)\n",
    "                self.in_size= embed_dim\n",
    "        else:\n",
    "            if len(input_size)!=3: \n",
    "                raise ValueError('the input list is 1 length')\n",
    "            else: \n",
    "                self.multi_emb=True\n",
    "                self.diag=self.med=self.oth=1\n",
    "\n",
    "        if self.time: self.in_size= self.in_size+1 \n",
    "               \n",
    "        \n",
    "        self.cell = nn.LSTM\n",
    "        self.rnn_c = self.cell(self.in_size, self.hidden_size, num_layers=self.n_layers, dropout= self.dropout_r, bidirectional=bii, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size*self.bi,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "      \n",
    "                            \n",
    "    #let's define this class method\n",
    "    def EmbedPatients_MB(self,mb_t, mtd): #let's define this\n",
    "        self.bsize=len(mb_t) ## no of pts in minibatch\n",
    "        \n",
    "        embedded = self.embed(mb_t)  ## Embedding for codes\n",
    "        embedded = torch.sum(embedded, dim=2) \n",
    "        if self.time:\n",
    "            mtd_t= Variable(torch.stack(mtd,0))\n",
    "            if use_cuda: \n",
    "                mtd_t.cuda()\n",
    "            out_emb= torch.cat((embedded,mtd_t),dim=2)\n",
    "        else:\n",
    "            out_emb= embedded\n",
    "        if use_cuda:\n",
    "            out_emb.cuda()        \n",
    "        return out_emb\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1:RNN & Variations: GRU, LSTM, Bi-RNN, Bi-GRU, Bi-LSTM\n",
    "class EHR_RNN(EHREmbeddings):\n",
    "    def __init__(self,input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='GRU',bii=False ,time=False, preTrainEmb='',packPadMode = True):\n",
    "       \tEHREmbeddings.__init__(self,input_size, embed_dim ,hidden_size, n_layers=n_layers, dropout_r=dropout_r, cell_type=cell_type, bii=bii, time=time , preTrainEmb=preTrainEmb, packPadMode=packPadMode)\n",
    "    #embedding function goes here \n",
    "    \n",
    "    def EmbedPatient_MB(self, input, mtd):\n",
    "        return EHREmbeddings.EmbedPatients_MB(self, input, mtd)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h_0 = Variable(torch.rand(self.n_layers*self.bi,self.bsize, self.hidden_size))\n",
    "        if use_cuda: \n",
    "            h_0.cuda()\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            result = (h_0,h_0)\n",
    "        else: \n",
    "            result = h_0\n",
    "        return result\n",
    "    \n",
    "    def forward(self, input, x_lens, mtd):\n",
    "        x_in  = self.EmbedPatient_MB(input, mtd) \n",
    "        ### uncomment the below lines if you like to initiate hidden to random instead of Zero which is the default\n",
    "        #h_0= self.init_hidden()\n",
    "        #if use_cuda: h_0.cuda()\n",
    "        if self.packPadMode: \n",
    "            x_inp = nn.utils.rnn.pack_padded_sequence(x_in,x_lens,batch_first=True)   \n",
    "            output, hidden = self.rnn_c(x_inp)#,h_0) \n",
    "        else:\n",
    "            output, hidden = self.rnn_c(x_in)#,h_0) \n",
    "        \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden=hidden[0]\n",
    "        if self.bi==2:\n",
    "            output = self.sigmoid(self.out(torch.cat((hidden[-2],hidden[-1]),1)))\n",
    "        else:\n",
    "            output = self.sigmoid(self.out(hidden[-1]))\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=[30000]\n",
    "embed_dim=128\n",
    "hidden_size=128\n",
    "n_layers=4\n",
    "dropout_r=0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_RNN(input_size= input_size, \n",
    "                          embed_dim=embed_dim, \n",
    "                          hidden_size= hidden_size,\n",
    "                          dropout_r=dropout_r,cell_type='LSTM') \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.metrics import roc_curve \n",
    "\n",
    "def calculate_auc(model, mbs_list,shuffle = True): \n",
    "    model.eval() \n",
    "    y_real =[]\n",
    "    y_hat= []\n",
    "    if shuffle: \n",
    "        random.shuffle(mbs_list)\n",
    "    for i,batch in enumerate(mbs_list):\n",
    "        sample, label_tensor, seq_l, mtd = batch\n",
    "        output = model(sample, seq_l, mtd)\n",
    "        y_hat.extend(output.cpu().data.view(-1).numpy())  \n",
    "        y_real.extend(label_tensor.cpu().data.view(-1).numpy())\n",
    "         \n",
    "    auc = roc_auc_score(y_real, y_hat)\n",
    "    return auc, y_real, y_hat \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Recurrent Neural Network\n",
    "\n",
    "Here you will find the table of the results for the RNN GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 0: Train_auc 0.9654, Valid_auc 0.4756, Test_auc 0.4952 ,Training Average_loss 0.6965\n",
      "\n",
      " Epoch 1: Train_auc 0.9989, Valid_auc 0.4749, Test_auc 0.4917 ,Training Average_loss 0.5323\n",
      "\n",
      " Epoch 2: Train_auc 1.0, Valid_auc 0.4876, Test_auc 0.4962 ,Training Average_loss 0.1989\n",
      "\n",
      " Epoch 3: Train_auc 1.0, Valid_auc 0.4854, Test_auc 0.4951 ,Training Average_loss 0.0379\n",
      "\n",
      " Epoch 4: Train_auc 1.0, Valid_auc 0.482, Test_auc 0.4958 ,Training Average_loss 0.0151\n",
      "\n",
      " Epoch 5: Train_auc 1.0, Valid_auc 0.4801, Test_auc 0.4968 ,Training Average_loss 0.0085\n",
      "\n",
      " Epoch 6: Train_auc 1.0, Valid_auc 0.4793, Test_auc 0.4983 ,Training Average_loss 0.0054\n",
      "\n",
      " Epoch 7: Train_auc 1.0, Valid_auc 0.4789, Test_auc 0.4991 ,Training Average_loss 0.0037\n",
      "\n",
      " Epoch 8: Train_auc 1.0, Valid_auc 0.4784, Test_auc 0.4995 ,Training Average_loss 0.0026\n",
      "\n",
      " Epoch 9: Train_auc 1.0, Valid_auc 0.4785, Test_auc 0.5 ,Training Average_loss 0.0019\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for ep in range(epochs):\n",
    "    current_loss = 0\n",
    "    train_loss =[]\n",
    "    plot_every = 5\n",
    "    n_iter = 0 \n",
    "    for i,batch in enumerate(train_mbs):\n",
    "        sample, label_tensor, seq_l, mtd = batch\n",
    "        \n",
    "        model.train() ## LR added Jul 10, that is the right implementation\n",
    "        model.zero_grad()\n",
    "        output = model(sample,seq_l, mtd)   \n",
    "        loss = criterion(output, label_tensor.ravel())    \n",
    "        loss.backward()   \n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        n_iter +=1\n",
    "    \n",
    "        if n_iter % plot_every == 0:\n",
    "            train_loss.append(current_loss/plot_every)\n",
    "            current_loss = 0  \n",
    "            \n",
    "\n",
    "    avg_loss = np.mean(train_loss)\n",
    "    shuffle= False\n",
    "    train_auc, _, _ = calculate_auc(model = model, mbs_list = train_mbs, shuffle = shuffle)\n",
    "    valid_auc, _, _ = calculate_auc(model = model, mbs_list = valid_mbs, shuffle = shuffle)\n",
    "    test_auc, _, _ = calculate_auc(model = model, mbs_list = test_mbs, shuffle = shuffle)\n",
    "#     valid_time = timeSince(valid_start)\n",
    "    print('\\n Epoch %s: Train_auc %s, Valid_auc %s, Test_auc %s ,Training Average_loss %s'%(ep, train_auc.round(4), valid_auc.round(4),test_auc.round(4), avg_loss.round(4)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
