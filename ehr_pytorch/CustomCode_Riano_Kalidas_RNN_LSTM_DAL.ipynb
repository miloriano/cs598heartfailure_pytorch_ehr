{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950f15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from EHRDataloader import EHRdataFromPickles, EHRdataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd5151",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "This part of the code loads the dataset, we use the EHRDataLoader.py\n",
    "The initial code could be found: https://github.com/ZhiGroup/pytorch_ehr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe0a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('1 file found. Data will be split into train, validation and test.')\n",
    "data = EHRdataFromPickles(root_dir = '../data/', \n",
    "                      file = 'toy.train', \n",
    "                      sort= False,\n",
    "                      test_ratio = 0.2, \n",
    "                      valid_ratio = 0.1,\n",
    "                      model='RNN') #No sort before splitting\n",
    "\n",
    "# Dataloader splits\n",
    "train, test, valid = data.__splitdata__()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df1b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get the patients labels, where 1: heart failure and 0: no heart failure\n",
    "labels=[]\n",
    "for ii in range(len(train)):\n",
    "    label=train[ii][1]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18b761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Distribution of the labels\n",
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3c6d",
   "metadata": {},
   "source": [
    "## 2. Sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26a214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## example dataset for patient 0 and visit 0\n",
    "patient=0\n",
    "visit=0\n",
    "print(\"Patient ID:\", train[patient][0])\n",
    "print(\"Heart Failure:\", train[patient][1])\n",
    "print(\"# of visits:\", len(train[patient][2]))\n",
    "\n",
    "print(f' list of visit_time (since last time): {train[patient][2][visit][0]}')\n",
    "print(f' list of codes corresponding to visit: {train[patient][2][visit][1]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39b3f4",
   "metadata": {},
   "source": [
    "# 3. Preprocess Data for Training\n",
    "\n",
    "This part of the code transforms the data which has the format described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acdaab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ddfc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "pack_pad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f5942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Understand EHRdataloader\n",
    "train_mbs = list(tqdm(EHRdataloader(train, batch_size = batch_size, packPadMode = pack_pad)))\n",
    "print (' creating the list of valid minibatches')\n",
    "valid_mbs = list(tqdm(EHRdataloader(valid, batch_size = batch_size, packPadMode = pack_pad)))\n",
    "print (' creating the list of test minibatches')\n",
    "test_mbs = list(tqdm(EHRdataloader(test, batch_size = batch_size, packPadMode = pack_pad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4fc1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbecfa3",
   "metadata": {},
   "source": [
    "# Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, dynamic_features, labels, last_features):\n",
    "        self._dynamic_features = dynamic_features\n",
    "        self._labels = labels\n",
    "        self._last_features = last_features\n",
    "        self._num_examples = labels.shape[0] \n",
    "        self._epoch_completed = 0\n",
    "        self._batch_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if batch_size > self.num_examples or batch_size <= 0:\n",
    "            # raise ValueError('The size of one batch: {} should be less than the total number of '\n",
    "            #                  'data: {}'.format(batch_size, self.num_examples))\n",
    "            batch_size = self._labels.shape[0]\n",
    "        if self._batch_completed == 0:\n",
    "            self._shuffle()\n",
    "        self._batch_completed += 1\n",
    "        start = self._index_in_epoch\n",
    "        if start + batch_size >= self.num_examples:\n",
    "            self._epoch_completed += 1\n",
    "            feature_rest_part = self._dynamic_features[start:self._num_examples]\n",
    "            label_rest_part = self._labels[start:self._num_examples]\n",
    "            last_feature_rest_part = self._last_features[start:self._num_examples]\n",
    "\n",
    "            self._shuffle()  \n",
    "            self._index_in_epoch = 0\n",
    "            return feature_rest_part, label_rest_part, last_feature_rest_part\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._dynamic_features[start:end], self._labels[start:end], self._last_features[start:end]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        index = np.arange(self._num_examples)\n",
    "        np.random.shuffle(index)\n",
    "        self._dynamic_features = self._dynamic_features[index]\n",
    "        self._labels = self._labels[index]\n",
    "        self._last_features = self._last_features[index]\n",
    "\n",
    "    @property\n",
    "    def dynamic_features(self):\n",
    "        return self._dynamic_features\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def last_features(self):\n",
    "        return self._last_features\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epoch_completed(self):\n",
    "        return self._epoch_completed\n",
    "\n",
    "    @property\n",
    "    def batch_completed(self):\n",
    "        return self._batch_completed\n",
    "\n",
    "    @epoch_completed.setter\n",
    "    def epoch_completed(self, value):\n",
    "        self._epoch_completed = value\n",
    "\n",
    "\n",
    "def read_data(max_times_of_visits=20):\n",
    "    dataset_label = pd.read_csv(\"resources/preprocessed_label.csv\", encoding='gbk')\n",
    "    patient_id_list_repeat = dataset_label.iloc[:, 0] \n",
    "    patient_id_list = []\n",
    "    [patient_id_list.append(i) for i in patient_id_list_repeat if not i in patient_id_list]\n",
    "\n",
    "    dataset_feature = pd.read_csv(\"resources/preprocessed_feature.csv\", encoding='gbk')\n",
    "    labels_on_visits = []\n",
    "    features_on_visit = []\n",
    "    features_of_last = []\n",
    "    for patient_id in patient_id_list:\n",
    "        one_visit_labels = dataset_label.loc[dataset_label['patient_id'] == patient_id].iloc[:, 2:].as_matrix()\n",
    "        one_visit_feature = dataset_feature.loc[dataset_feature['patient_id'] == patient_id].iloc[:, 3:].as_matrix()\n",
    "       \n",
    "        if max_times_of_visits < one_visit_labels.shape[0] - 1:\n",
    "            labels_on_visits.append(one_visit_labels[max_times_of_visits - 1])  \n",
    "            features_on_visit.append(one_visit_feature[0:max_times_of_visits])  \n",
    "            features_of_last.append(one_visit_feature[max_times_of_visits])  \n",
    "        else:\n",
    "            labels_on_visits.append(one_visit_labels[one_visit_labels.shape[0] - 2]) \n",
    "            features_on_visit.append(one_visit_feature[0:one_visit_labels.shape[0] - 1]) \n",
    "            features_of_last.append(one_visit_feature[one_visit_feature.shape[0] - 1])  \n",
    "\n",
    "    all_labels_on_visits = np.array(labels_on_visits)\n",
    "   \n",
    "    binary_labels_of_revisits = np.expand_dims(np.sign(np.sum(all_labels_on_visits[:, 5 + 0 * 11:10 + 0 * 11], 1)), 1)\n",
    "    # binary_labels_of_revisits = np.expand_dims(np.sign(np.sum(all_labels_on_visits[:, 10 + 0 * 11:11 + 0 * 11], 1)), 1)\n",
    "    # labels_of_revisits = all_labels_on_visits[:, 5:11]\n",
    "    # labels_of_revisits = np.c_[labels_of_revisits, binary_labels_of_revisits]\n",
    "    features_on_visit = list(\n",
    "        map(lambda x: np.pad(x, ((0, max_times_of_visits - x.shape[0]), (0, 0)), 'constant', constant_values=0),\n",
    "            features_on_visit))\n",
    "    features_on_visit = np.stack(features_on_visit)\n",
    "    features_of_last = np.stack(features_of_last)\n",
    "    return DataSet(features_on_visit, binary_labels_of_revisits, features_of_last)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = read_data(7)\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90fce9",
   "metadata": {},
   "source": [
    "# Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad06329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "\n",
    "class LstmGanModel(object):\n",
    "    def __init__(self, num_features, time_steps,\n",
    "                 lstm_size=200, n_output=1, batch_size=64,\n",
    "                 epochs=1000,\n",
    "                 output_n_epoch=10,\n",
    "                 learning_rate=0.01, max_loss=0.5, max_pace=0.01, lasso=0.0, ridge=0.0,\n",
    "                 optimizer='adam', name='LSTM-GAN'):\n",
    "        self._num_features = num_features\n",
    "        self._epochs = epochs\n",
    "        self._name = name\n",
    "        self._batch_size = batch_size\n",
    "        self._output_n_epoch = output_n_epoch\n",
    "        self._lstm_size = lstm_size\n",
    "        self._n_output = n_output\n",
    "        self._time_steps = time_steps\n",
    "        self._max_loss = max_loss\n",
    "        self._max_pace = max_pace\n",
    "        self._lasso = lasso\n",
    "        self._ridge = ridge\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "        print(\"learning_rate=\", learning_rate, \"max_loss=\", max_loss, \"max_pace=\", max_pace, \"lasso=\", lasso, \"ridge=\",\n",
    "              ridge)\n",
    "\n",
    "        self._graph_definition()\n",
    "\n",
    "    def _graph_definition(self):\n",
    "        self._placeholder_definition() \n",
    "        self._sess = tf.Session() \n",
    "        with tf.variable_scope('generator'):\n",
    "            self._hidden_layer() \n",
    "            self._output = tf.contrib.layers.fully_connected(self._hidden_rep, self._n_output,\n",
    "                                                             activation_fn=tf.identity)\n",
    "            self._pred = tf.nn.sigmoid(self._output, name=\"pred\") \n",
    "            self._hidden_state_decoder() \n",
    "   \n",
    "\n",
    "        with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
    "            self._fake_logits, self._fake_pred = self._hidden_layer_of_discriminator(self._predicted_last_x)\n",
    "            self._real_logits, self._real_pred = self._hidden_layer_of_discriminator(self._last_x)\n",
    "\n",
    "        self._gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self._fake_logits,\n",
    "                                                                                labels=tf.ones_like(self._fake_logits)))\n",
    "        fake_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self._fake_logits, labels=tf.zeros_like(self._fake_logits)))\n",
    "        real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self._real_logits,\n",
    "                                                                           labels=tf.ones_like(self._real_logits)))\n",
    "        self._loss_of_discriminator = tf.add(fake_loss, real_loss)\n",
    "\n",
    "        self._loss()\n",
    "        self._loss_regulation()\n",
    "\n",
    "        train_vars = tf.trainable_variables()\n",
    "\n",
    "        gen_vars = [var for var in train_vars if var.name.startswith('generator')]\n",
    "\n",
    "        dis_vars = [var for var in train_vars if var.name.startswith('discriminator')]\n",
    "        self._generator_train_op = self._optimizer.minimize(self._loss_of_whole_generator, var_list=gen_vars)  # 定义训练\n",
    "        self._discriminator_train_op = self._optimizer.minimize(self._loss_of_discriminator, var_list=dis_vars)\n",
    "\n",
    "    def _hidden_layer_of_discriminator(self, samples):\n",
    "        logits = tf.layers.dense(samples, 1)\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        return logits, pred\n",
    "\n",
    "    def _placeholder_definition(self):\n",
    "        self._x = tf.placeholder(tf.float32, [None, self._time_steps, self._num_features], 'input')\n",
    "        self._y = tf.placeholder(tf.float32, [None, self._n_output], 'label')\n",
    "        self._last_x = tf.placeholder(tf.float32, [None, self._num_features], \"true_last_x\")\n",
    "        self._keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    def _hidden_layer(self):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(self._lstm_size)\n",
    "        init_state = lstm.zero_state(tf.shape(self._x)[0], tf.float32)  # 全零向量\n",
    "\n",
    "        mask, length = self._length() \n",
    "        self._hidden, self._final_state = tf.nn.dynamic_rnn(lstm,\n",
    "                                                            self._x,\n",
    "                                                            sequence_length=length,\n",
    "                                                            initial_state=init_state)\n",
    "        self._hidden_rep = self._final_state.h\n",
    "\n",
    "    def _hidden_state_decoder(self):\n",
    "        # W_decoders = tf.Variable(xavier_init(self._n_output, self._lstm_size, self._num_features))\n",
    "        # b_decoders = tf.Variable(tf.zeros(self._n_output))\n",
    "        # decoders = tf.keras.backend.dot(self._hidden_rep, W_decoders) + tf.tile(tf.expand_dims(b_decoders, 1),\n",
    "        #                                                                         [1, self._num_features])\n",
    "        # self._predicted_last_x = tf.reshape(tf.matmul(tf.expand_dims(self._pred, 1), decoders),\n",
    "        #                                     [-1, self._num_features])\n",
    "        self._predicted_last_x = tf.contrib.layers.fully_connected(self._hidden_rep, self._num_features,\n",
    "                                                                   activation_fn=tf.identity)\n",
    "\n",
    "    def _loss(self):\n",
    "        self._loss_classification = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=self._y, logits=self._output))\n",
    "        self._loss_of_whole_generator = self._loss_classification + 1*self._gen_loss\n",
    "\n",
    "    def _loss_regulation(self):\n",
    "        if self._lasso != 0:\n",
    "            for trainable_variables in tf.trainable_variables(\"generator\"):\n",
    "                self._loss_of_whole_generator += tf.contrib.layers.l1_regularizer(self._lasso)(trainable_variables)\n",
    "            for trainable_variables in tf.trainable_variables(\"discriminator\"):\n",
    "                self._loss_of_discriminator += tf.contrib.layers.l1_regularizer(self._lasso)(trainable_variables)\n",
    "        if self._ridge != 0:\n",
    "            for trainable_variables in tf.trainable_variables(\"generator\"):\n",
    "                self._loss_of_whole_generator += tf.contrib.layers.l2_regularizer(self._ridge)(trainable_variables)\n",
    "            for trainable_variables in tf.trainable_variables(\"discriminator\"):\n",
    "                self._loss_of_discriminator += tf.contrib.layers.l2_regularizer(self._ridge)(trainable_variables)\n",
    "\n",
    "    def _length(self):\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self._x), 2))  \n",
    "        length = tf.reduce_sum(mask, 1)  \n",
    "        length = tf.cast(length, tf.int32)  \n",
    "        return mask, length\n",
    "\n",
    "    def _train_single_batch(self, dynamic_features, labels, last_features):\n",
    "        self._sess.run(self._generator_train_op, feed_dict={self._x: dynamic_features,\n",
    "                                                            self._y: labels,\n",
    "                                                            self._last_x: last_features})\n",
    "        self._sess.run(self._discriminator_train_op, feed_dict={self._x: dynamic_features,\n",
    "                                                                self._y: labels,\n",
    "                                                                self._last_x: last_features})\n",
    "\n",
    "    def _loss_on_training_set(self, data_set):\n",
    "        return self._sess.run(\n",
    "            (self._loss_of_whole_generator, self._loss_classification, self._gen_loss, self._loss_of_discriminator),\n",
    "            feed_dict={self._x: data_set.dynamic_features,\n",
    "                       self._y: data_set.labels,\n",
    "                       self._last_x: data_set.last_features})\n",
    "\n",
    "    def fit(self, train_set, test_set):\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        train_set.epoch_completed = 0\n",
    "\n",
    "        for c in tf.trainable_variables(self._name):\n",
    "            print(c.name)\n",
    "\n",
    "        print(\"auc\\tepoch\\tloss\\tloss_diff\\tloss_classification\\tloss_generator\\tloss_discriminator\")\n",
    "        logged = set()\n",
    "        loss = 0\n",
    "        while train_set.epoch_completed < self._epochs:\n",
    "            dynamic_features, labels, last_features = train_set.next_batch(self._batch_size)\n",
    "\n",
    "            if train_set.batch_completed == 1:\n",
    "                loss = self.show_training(train_set, test_set, loss)  \n",
    "\n",
    "            self._train_single_batch(dynamic_features, labels, last_features)\n",
    "            if train_set.epoch_completed != 0 and train_set.epoch_completed % self._output_n_epoch == 0 and train_set.epoch_completed not in logged:\n",
    "                logged.add(train_set.epoch_completed)\n",
    "                loss = self.show_training(train_set, test_set, loss)\n",
    "\n",
    "    def predict(self, test_set):\n",
    "        return np.expand_dims(\n",
    "            self._sess.run(self._pred, feed_dict={self._x: test_set.dynamic_features, self._keep_prob: 1})[:, -1],\n",
    "            1)\n",
    "\n",
    "    def show_training(self, train_set, test_set, loss):\n",
    "        loss_prev = loss\n",
    "        loss, loss_classification, loss_generator, loss_discriminator = self._loss_on_training_set(train_set)\n",
    "        loss_diff = loss_prev - loss\n",
    "        y_score = self.predict(test_set)  \n",
    "        auc = roc_auc_score(test_set.labels[:, -1], y_score)\n",
    "        print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(auc, train_set.epoch_completed, loss, loss_diff,\n",
    "                                                      loss_classification, loss_generator, loss_discriminator,\n",
    "                                                      time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())))\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    def close(self):\n",
    "        self._sess.close()\n",
    "        tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed6f8c",
   "metadata": {},
   "source": [
    "# Evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a156cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, roc_curve  # roc计算曲线\n",
    "from sklearn.model_selection import StratifiedShuffleSplit  \n",
    "import tensorflow as tf\n",
    "from models import LstmGanModel\n",
    "from data import read_data, DataSet\n",
    "\n",
    "\n",
    "class ExperimentSetup(object):\n",
    "    kfold = 5\n",
    "    batch_size = 64\n",
    "    encoder_size = 50\n",
    "    lstm_size = 128\n",
    "    learning_rate = 0.001\n",
    "    epochs = 10\n",
    "    output_n_epochs = 1\n",
    "    max_times_of_visits = 3\n",
    "    ridge_l2 = 0.001\n",
    "\n",
    "\n",
    "def evaluate(test_index, y_label, y_score, file_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param test_index\n",
    "    :param y_label: 测试样本的真实标签 true label of test-set\n",
    "    :param y_score: 测试样本的预测概率 predicted probability of test-set\n",
    "    :param file_name: 输出文件路径    path of output file\n",
    "    \"\"\"\n",
    "    # TODO \n",
    "    wb = xlwt.Workbook(file_name + '.xls')\n",
    "    table = wb.add_sheet('Sheet1')\n",
    "    table_title = [\"test_index\", \"label\", \"prob\", \"pre\", \" \", \"fpr\", \"tpr\", \"thresholds\", \" \",\n",
    "                   \"acc\", \"auc\", \"recall\", \"precision\", \"f1-score\", \"threshold\"]\n",
    "    for i in range(len(table_title)):\n",
    "        table.write(0, i, table_title[i])\n",
    "\n",
    "    auc = roc_auc_score(y_label, y_score)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_label, y_score, pos_label=1)\n",
    "    threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "\n",
    "    for i in range(len(fpr)):\n",
    "        table.write(i + 1, table_title.index(\"fpr\"), fpr[i])\n",
    "        table.write(i + 1, table_title.index(\"tpr\"), tpr[i])\n",
    "        table.write(i + 1, table_title.index(\"thresholds\"), float(thresholds[i]))\n",
    "    table.write(1, table_title.index(\"threshold\"), float(threshold))\n",
    "\n",
    "    y_pred_label = (y_score >= threshold) * 1\n",
    "    acc = accuracy_score(y_label, y_pred_label)\n",
    "    recall = recall_score(y_label, y_pred_label)\n",
    "    precision = precision_score(y_label, y_pred_label)\n",
    "    f1 = f1_score(y_label, y_pred_label)\n",
    "\n",
    "    for i in range(len(test_index)):\n",
    "        table.write(i + 1, table_title.index(\"test_index\"), int(test_index[i]))\n",
    "        table.write(i + 1, table_title.index(\"label\"), int(y_label[i]))\n",
    "        table.write(i + 1, table_title.index(\"prob\"), float(y_score[i]))\n",
    "        table.write(i + 1, table_title.index(\"pre\"), int(y_pred_label[i]))\n",
    "\n",
    "    # write metrics\n",
    "    table.write(1, table_title.index(\"auc\"), float(auc))\n",
    "    table.write(1, table_title.index(\"acc\"), float(acc))\n",
    "    table.write(1, table_title.index(\"recall\"), float(recall))\n",
    "    table.write(1, table_title.index(\"precision\"), float(precision))\n",
    "    table.write(1, table_title.index(\"f1-score\"), float(f1))\n",
    "\n",
    "    wb.save(file_name + \".xls\")\n",
    "\n",
    "\n",
    "def model_experiments(model, data_set, result_file):\n",
    "    dynamic_features = data_set.dynamic_features\n",
    "    labels = data_set.labels\n",
    "    last_features = data_set.last_features\n",
    "    kf = sklearn.model_selection.StratifiedKFold(n_splits=ExperimentSetup.kfold, shuffle=False)\n",
    "\n",
    "    # n_output = labels.shape[1]  # classes\n",
    "    n_output = 1  # classes\n",
    "\n",
    "    tol_test_idx = np.zeros(0, dtype=np.int32)\n",
    "    tol_pred = np.zeros(shape=(0, n_output))\n",
    "    tol_label = np.zeros(shape=(0, n_output), dtype=np.int32)\n",
    "    i = 1\n",
    "    for train_idx, test_idx in kf.split(X=data_set.dynamic_features, y=data_set.labels[:, -1]):\n",
    "        train_dynamic = dynamic_features[train_idx]\n",
    "        train_y = labels[train_idx]\n",
    "        train_last_features = last_features[train_idx]\n",
    "        train_set = DataSet(train_dynamic, train_y, train_last_features)\n",
    "\n",
    "        test_dynamic = dynamic_features[test_idx]\n",
    "        test_y = labels[test_idx]\n",
    "        test_last_features = last_features[test_idx]\n",
    "        test_set = DataSet(test_dynamic, test_y, test_last_features)\n",
    "        print(\"learning_rate = \", ExperimentSetup.learning_rate)\n",
    "        model.fit(train_set, test_set)\n",
    "\n",
    "        y_score = model.predict(test_set)\n",
    "        tol_test_idx = np.concatenate((tol_test_idx, test_idx))\n",
    "        tol_pred = np.vstack((tol_pred, y_score))\n",
    "        tol_label = np.vstack((tol_label, np.expand_dims(test_y[:, -1], 1)))\n",
    "        print(\"Cross validation: {} of {}\".format(i, ExperimentSetup.kfold),\n",
    "              time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "        i += 1\n",
    "        # evaluate(test_y, y_score, result_file)\n",
    "\n",
    "    model.close()\n",
    "    # with open(result_file, 'a', newline='') as csv_file:\n",
    "    #     f_writer = csv.writer(csv_file, delimiter=',')\n",
    "    #     f_writer.writerow([])\n",
    "    # return evaluate(tol_label, tol_pred, result_file)\n",
    "    return evaluate(tol_test_idx, tol_label, tol_pred, result_file)\n",
    "\n",
    "\n",
    "def denoising_auto_encoder_lstm_decoder_model_experiments(result_file):\n",
    "    data_set = toy.train\n",
    "    dynamic_features = data_set.dynamic_features\n",
    "    labels = data_set.labels\n",
    "\n",
    "    num_features = dynamic_features.shape[2]\n",
    "    time_steps = dynamic_features.shape[1]\n",
    "    n_output = labels.shape[1]\n",
    "\n",
    "    model = LstmGanModel(num_features=num_features,\n",
    "                         time_steps=time_steps,\n",
    "                         lstm_size=ExperimentSetup.lstm_size,\n",
    "                         n_output=n_output,\n",
    "                         batch_size=ExperimentSetup.batch_size,\n",
    "                         epochs=ExperimentSetup.epochs,\n",
    "                         output_n_epoch=ExperimentSetup.output_n_epochs,\n",
    "                         learning_rate=ExperimentSetup.learning_rate,\n",
    "                         ridge=ExperimentSetup.ridge_l2,\n",
    "                         optimizer=tf.train.AdamOptimizer(ExperimentSetup.learning_rate))\n",
    "    return model_experiments(model, data_set, result_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # basic_lstm_model_experiments('resources/save/basic_lstm.csv')\n",
    "    # lstm_with_static_feature_model_experiments(\"resources/save/lstm_with_static.csv\")\n",
    "    # bidirectional_lstm_model_experiments('resources/save/bidirectional_lstm.csv')\n",
    "    for i_times in range(20):\n",
    "        # print(\"mlp_bi-lstm_att\")\n",
    "        # bi_lstm_attention_model_experiments('result_qx/MLA1-' + str(i_times + 1), True, True)\n",
    "        save_path = \"heart_3mon_3times_10epoch_1gen_LR0.001_L0.001\"\n",
    "        # save_path = \"all_cause_24mon\"\n",
    "        print(\"save to \" + save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        # print(\"DAE-LSTM\")\n",
    "        # denoising_auto_encoder_lstm_model_experiments(\n",
    "        #     save_path + '/DAE-LSTM-DROPOUT-50DAEsize-20epoch-0.001lr-1-' + str(i_times + 1))\n",
    "        #\n",
    "        print(\"LSTM_GAN\")\n",
    "        denoising_auto_encoder_lstm_decoder_model_experiments(\n",
    "            save_path + '/LSTM_GAN_1-' + str(i_times + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5abc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
